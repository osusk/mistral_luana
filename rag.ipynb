{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a54b7d",
   "metadata": {},
   "source": [
    "# RAG for ColaborEJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab92dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gui\\Documents\\USP\\TCC\\mistral_luana\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3667: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abdef6d40534f6d871947b6cb87243f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "nb_4bit_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"rhaymison/Mistral-portuguese-luana-7b\",\n",
    "    quantization_config=nb_4bit_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rhaymison/Mistral-portuguese-luana-7b\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf04e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\Gui\\AppData\\Local\\Temp\\ipykernel_45124\\4081304555.py:12: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model, \n",
    "                device_map='auto',\n",
    "                tokenizer=tokenizer, \n",
    "                use_cache = True,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty = 1.2,\n",
    "                max_new_tokens=2000)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902eb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer\n",
    "the question. If you don't know the answer, say that you\n",
    "don't know. DON'T MAKE UP ANYTHING.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1d0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a highly sophisticated artificial intelligence specialized in digital literacy,\n",
    "with a focus on Youth and Adult Education, assisting teachers in creating lesson plans\n",
    "that meet the needs of their students.\n",
    "Use the contextual excerpts provided to answer the question and develop a lesson plan to assist the teacher.\n",
    "Be respectful and pay attention to Brazilian Portuguese spelling.\n",
    "If you do not know the answer, simply say that you do not know. DO NOT MAKE ANYTHING UP.\n",
    "Use four sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7578bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_splitter():\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,\n",
    "                                               chunk_overlap = 500,\n",
    "                                               length_function=len,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "    return text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ebbc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gui\\AppData\\Local\\Temp\\ipykernel_45124\\640221044.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings_function():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name =\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    return embeddings\n",
    "embedding_function = get_embeddings_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f35d0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "\n",
    "    # Create a list of unique ids for each document based on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "    \n",
    "    # Ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    unique_chunks = [] \n",
    "    for chunk, id in zip(chunks, ids):     \n",
    "        if id not in unique_ids:       \n",
    "            unique_ids.add(id)\n",
    "            unique_chunks.append(chunk) \n",
    "\n",
    "    # Create a new Chroma database from the documents\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks, \n",
    "                                        ids=list(unique_ids),\n",
    "                                        embedding=embedding_function, \n",
    "                                        persist_directory = vectorstore_path)\n",
    "\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7255664a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.32553651341390677}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator = \"embedding_distance\",\n",
    "                           embeddings = embedding_function)\n",
    "evaluator.evaluate_strings(prediction= \"cat\", reference=\"Animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d79366fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_pdf_file(pdf_file):\n",
    "    text_splitter = get_text_splitter()\n",
    "    chunks = text_splitter.split_documents(load_pdf(pdf_file))\n",
    "    # Create vectorstore\n",
    "    vectorstore = create_vectorstore(chunks=chunks, \n",
    "                                    embedding_function=embedding_function, \n",
    "                                    vectorstore_path=\"vectorstore_diretivas\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9bd0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever and get relevant chunks\n",
    "def get_retriver(question, vectorstore):\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "    relevant_chunks = retriever.invoke(question)\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59cd9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(question, vectorstore):\n",
    "    relevant_chunks = get_retriver(question = question, vectorstore = vectorstore)\n",
    "    # Concatenate context text\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "    # Create prompt\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, \n",
    "                                    question = question)\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad6965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gui\\Documents\\USP\\TCC\\mistral_luana\\venv\\Lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n",
      "C:\\Users\\Gui\\AppData\\Local\\Temp\\ipykernel_45124\\2260934871.py:24: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "pdfs_file = [\"diretivas\\REquestoes_prinpicais.pdf\",\n",
    "             \"diretivas\\questoes_alternativas.pdf\",\n",
    "             \"diretivas\\Exemplo de plano de aula.pdf\"]\n",
    "\n",
    "all_docs = []\n",
    "for pdf in pdfs_file:\n",
    "    loader = PyPDFLoader(pdf, extraction_mode=\"layout\")\n",
    "    pages = loader.load()\n",
    "    all_docs.extend(pages)\n",
    "\n",
    "# Criar o text splitter uma vez\n",
    "text_splitter = get_text_splitter()\n",
    "\n",
    "# Dividir os documentos em chunks\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Criar o vectorstore persistente com todos os chunks\n",
    "vectorstore = create_vectorstore(\n",
    "    chunks=chunks,\n",
    "    embedding_function=embedding_function,\n",
    "    vectorstore_path=\"vectorstore_diretivas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7337eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A pergunta está relacionada ao componente \"Efficacia\" no modelo de habilidades multifacetadas (MHP), pois implica a capacidade de uma parte de trabalhar de forma eficaz na tarefa. Os sensors citados em questões como \"Qual sensor é necessário para aprender o código de comunicação?\" ou \"Quais informações de conscientização o participante deve armazenar?\" são necessárias para desempenhar a função esperada dentro do modelo. Assim, a pergunta aborda diretamente o atributo de qualidade relevante e ajuda o participante a criar seu plano de aula.\n"
     ]
    }
   ],
   "source": [
    "answer = generate(\"\"\"Na tabela 1, a pergunta 'Quais sensores são necessários para aprender o código de comunicação? \n",
    "O participante consegue usar esses sensores?'\n",
    "                  esta relacionada a qual componente do MHP?\n",
    "                    \"\"\",\n",
    "                   vectorstore)\n",
    "\n",
    "print(answer.split(\"Answer:\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd6e89",
   "metadata": {},
   "source": [
    "# tentando uma outra abordagem\n",
    "## Usando langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ce506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "    # Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vectorstore.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt_template.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd750a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer\n",
      "the question. If you don't know the answer, say that you\n",
      "don't know. DON'T MAKE UP ANYTHING.\n",
      "Use three sentences maximum and keep the answer concise.\n",
      "Question: Quais são as questões de acessibilidade que um jovem com autismo pode encontrar? \n",
      "                    Quais as possíveis soluções?\n",
      "                    Faça um novo plano de aula baseado nas questões anteriores e usando o Plano de Aula como exemplo.\n",
      "                     \n",
      "Context: Tabela 5: Alternativas a serem exploradas se um desafio de comunicação for identificado.\n",
      "Questão Alternativas\n",
      "Quais sensores são necessários para aprender o código de comunicação? \n",
      "O participante consegue usar esses sensores?- É possível tornar esse código disponível por meio de outros sensores?\n",
      "- Existe algum conversor automático que traduz o material de aprendizado \n",
      "para outros sensores?\n",
      "- Há pessoas disponíveis para fazer essa conversão?\n",
      "- As ferramentas e o método usados para ensinar o código são apropriados?\n",
      "Quantas regras de comunicação devem ser armazenadas? O participante\n",
      "consegue armazenar essa quantidade de informações?- São necessárias todas as regras para a tarefa específica?\n",
      "- O código pode ser simplificado?\n",
      "- É possível treinar outros participantes para cooperar sem algumas regras \n",
      "de comunicação?\n",
      "- Há suporte computacional disponível para complementar isso?\n",
      "O participante consegue memorizar o código durante toda a tarefa?- É possível fazer interrupções durante a tarefa para lembrar partes do código?\n",
      "- Existe algum suporte computacional para complementar isso?\n",
      "O participante consegue codificar as mensagens corretamente?- O código pode ser simplificado?\n",
      "- Existe suporte computacional para complementar isso?\n",
      "O participante consegue enviar mensagens usando os músculos?- Existem outros códigos que podem ser usados (por exemplo, gestos em \n",
      "substituição às palavras)? Se sim, esses códigos são suficientes para transmitir a mensagem?\n",
      "\n",
      "- Pode o software de grupo coordenar a participação?\n",
      "- Há suporte computacional para complementar isso?\n",
      "O participante é capaz de respeitar o momento adequado para agir e se comunicar?\n",
      "- Pode o groupware informar quando o participante está autorizado a agir e se comunicar?\n",
      "- Pode alguém informar quando o participante está autorizado a agir e se comunicar?\n",
      "- Investigar como o desvio do comportamento esperado pelo participante prejudica a atividade colaborativa.O participante possui as habilidades cognitivas necessárias para relacionar \n",
      "informações de conscientização da atividade colaborativa com regras de \n",
      "coordenação?\n",
      "O tempo necessário para coordenar é adequado à dinâmica da tarefa?\n",
      "- As dinâmicas da tarefa podem ser modificadas? Caso contrário, a inclusão em tal tarefa pode \n",
      "não ser possível?O tempo necessário para processar as ordens de coordenação é adequado à \n",
      "dinâmica da tarefa?\n",
      "O processo de coordenação é sem esforço?- O participante está ciente disso?\n",
      "- Isso pode afetar a saúde do participante? O processo de coordenação está seguro contra riscos para o participante?\n",
      "\n",
      "- Outros participantes podem enviar essas mensagens para outros sensores \n",
      "(por exemplo, comunicando verbalmente o que está sendo apontado)?\n",
      "O tempo necessário para processar as mensagens é adequado \n",
      "à dinâmica da tarefa?\n",
      "- É possível modificar a dinâmica da tarefa? Caso contrário, a inclusão nessa tarefa \n",
      "pode não ser possível?O tempo necessário para produzir mensagens é adequado \n",
      "à dinâmica da tarefa?\n",
      "O processo de comunicação é sem esforço?- O participante está ciente disso?\n",
      "- Isso pode afetar a saúde do participante? O processo de comunicação é seguro e sem riscos para o participante?\n",
      "\n",
      "Tabela 6: Alternativas que podem ser exploradas se um desafio de coordenação for identificado.\n",
      "Questão Alternativas\n",
      "Quais sensores são necessários para aprender as regras de coordenação? \n",
      "O participante é capaz de usar esses sensores?- É possível disponibilizar este código através de outros sensores?\n",
      "- Existe algum conversor automático que traduza o material de aprendizagem para outros \n",
      "sensores?\n",
      "- Há pessoas disponíveis para fazer essa conversão?\n",
      "- As ferramentas e métodos usados para ensinar as regras são apropriados?\n",
      "Quantas regras de coordenação devem ser armazenadas? O participante é capaz \n",
      "de armazenar essa quantidade de informação?- Todas as regras são necessárias para a tarefa específica?\n",
      "- Elas podem ser simplificadas?\n",
      "- É possível treinar outros participantes para cooperar sem algumas regras de comunicação?\n",
      "- Há suporte computacional para complementar isso?\n",
      "O participante é capaz de memorizar as regras de coordenação durante a tarefa?- É possível fazer interrupções durante a tarefa para lembrar partes das regras?\n",
      "- Pode o software de grupo coordenar a participação?\n",
      "- Há suporte computacional para complementar isso?\n",
      "O participante é capaz de respeitar o momento adequado para agir e se comunicar?\n",
      "- Pode o groupware informar quando o participante está autorizado a agir e se comunicar?\n",
      "- Pode alguém informar quando o participante está autorizado a agir e se comunicar? \n",
      "Answer: Um jovem com autismo pode enfrentar vários desafios na área de acessibilidade, incluindo diferenças nos sentidos, dificuldades de coordenação ou falta de compreensão social. Algumas alternativas potenciais para superar estes desafios incluem a utilização de diferentes sensores, aplicação de conversors automatizados, a incorporação de sistemas de ajuda para ajudar no processo de codificação, modificação dos códigos existentes para facilitar a comunicação e implementação de assistências sociais para melhorar a capacidade de coordenação entre indivíduos. Em cada situação, uma abordagem individualizada que considera as necessidades únicas do indivíduo com autismo é crucial para garantir sua inclusive participação.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"\"\"Quais são as questões de acessibilidade que um jovem com autismo pode encontrar? \n",
    "                    Quais as possíveis soluções?\n",
    "                    Faça um novo plano de aula baseado nas questões anteriores e usando o Plano de Aula como exemplo.\n",
    "                    \"\"\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
